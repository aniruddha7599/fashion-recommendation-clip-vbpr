{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9dd53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting pre-check for missing images...\n",
      "INFO:__main__:Pre-check: Total missing or invalid images: 0\n",
      "ITC Fine-tuning Epoch 1/15: 100%|██████████| 364/364 [02:06<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 1/15 Completed, Average Loss: 2.1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 2/15: 100%|██████████| 364/364 [02:07<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 2/15 Completed, Average Loss: 1.5733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 3/15: 100%|██████████| 364/364 [02:39<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 3/15 Completed, Average Loss: 1.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 4/15: 100%|██████████| 364/364 [02:38<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 4/15 Completed, Average Loss: 1.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 5/15: 100%|██████████| 364/364 [02:35<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 5/15 Completed, Average Loss: 1.1076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 6/15: 100%|██████████| 364/364 [02:33<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 6/15 Completed, Average Loss: 0.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 7/15: 100%|██████████| 364/364 [01:53<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 7/15 Completed, Average Loss: 0.4246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 8/15: 100%|██████████| 364/364 [02:24<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 8/15 Completed, Average Loss: 0.3707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 9/15: 100%|██████████| 364/364 [02:21<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 9/15 Completed, Average Loss: 0.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 10/15: 100%|██████████| 364/364 [02:27<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 10/15 Completed, Average Loss: 0.3183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 11/15: 100%|██████████| 364/364 [02:38<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 11/15 Completed, Average Loss: 0.2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 12/15: 100%|██████████| 364/364 [02:34<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 12/15 Completed, Average Loss: 0.2818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 13/15: 100%|██████████| 364/364 [02:23<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 13/15 Completed, Average Loss: 0.2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 14/15: 100%|██████████| 364/364 [02:12<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 14/15 Completed, Average Loss: 0.2606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 15/15: 100%|██████████| 364/364 [01:50<00:00,  3.29it/s]\n",
      "INFO:__main__:Total missing or invalid images: 0\n",
      "INFO:__main__:Total missing or empty texts: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITC Fine-tuning Epoch 15/15 Completed, Average Loss: 0.2728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 364/364 [01:38<00:00,  3.71it/s]\n",
      "VBPR Training Epoch 1/10: 100%|██████████| 76/76 [00:43<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 1/10 Completed, Average Loss: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 2/10: 100%|██████████| 76/76 [00:44<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 2/10 Completed, Average Loss: 0.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 3/10: 100%|██████████| 76/76 [00:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 3/10 Completed, Average Loss: 0.7903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 4/10: 100%|██████████| 76/76 [00:48<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 4/10 Completed, Average Loss: 0.6713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 5/10: 100%|██████████| 76/76 [00:44<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 5/10 Completed, Average Loss: 0.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 6/10: 100%|██████████| 76/76 [00:49<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 6/10 Completed, Average Loss: 0.4105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 7/10: 100%|██████████| 76/76 [00:47<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 7/10 Completed, Average Loss: 0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 8/10: 100%|██████████| 76/76 [00:49<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 8/10 Completed, Average Loss: 0.1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 9/10: 100%|██████████| 76/76 [00:43<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 9/10 Completed, Average Loss: 0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 10/10: 100%|██████████| 76/76 [00:41<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBPR Training Epoch 10/10 Completed, Average Loss: 0.0861\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "df_path = 'C:/Users/Aniruddha shinde/DL Project/Local/preprocessed_fashion_data.pkl'\n",
    "user_item_path = 'C:/Users/Aniruddha shinde/DL Project/Local/user_item_interactions.pkl'\n",
    "new_image_dir = 'C:/Users/Aniruddha shinde/DL Project/Local/images'\n",
    "save_dir = 'C:/Users/Aniruddha shinde/DL Project/Local/clip_fashion_model'\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_pickle(df_path)\n",
    "with open(user_item_path, 'rb') as f:\n",
    "    user_item_data = pickle.load(f)\n",
    "\n",
    "# Convert user_item_data to DataFrame and aggregate duplicates\n",
    "rows = []\n",
    "for user_id, interactions in user_item_data.items():\n",
    "    for item_id, description, image_paths in interactions:\n",
    "        rows.append({'user_id': user_id, 'item_id': item_id, 'description': description, 'image_paths': image_paths})\n",
    "user_item_df = pd.DataFrame(rows)\n",
    "user_item_df = user_item_df.groupby(['user_id', 'item_id']).agg({\n",
    "    'description': 'first',\n",
    "    'image_paths': lambda x: list(set(sum(x, [])))\n",
    "}).reset_index()\n",
    "\n",
    "# Map user IDs to indices\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(user_item_df['user_id'].unique())}\n",
    "user_item_df['user_idx'] = user_item_df['user_id'].map(user_mapping)\n",
    "\n",
    "# Map item IDs to df indices using 'asin'\n",
    "item_mapping = dict(zip(df['asin'], df.index))\n",
    "user_item_df['item_idx'] = user_item_df['item_id'].map(item_mapping)\n",
    "\n",
    "# Pre-check for missing images\n",
    "def precheck_images(df, image_dir):\n",
    "    missing_images = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        image_path = row['image_paths'][0] if row['image_paths'] else None\n",
    "        if not image_path or not isinstance(image_path, str) or not os.path.exists(image_path):\n",
    "            missing_images += 1\n",
    "            logger.warning(f\"Missing or invalid image path at index {idx}: {image_path}\")\n",
    "    logger.info(f\"Pre-check: Total missing or invalid images: {missing_images}\")\n",
    "\n",
    "logger.info(\"Starting pre-check for missing images...\")\n",
    "precheck_images(df, new_image_dir)\n",
    "\n",
    "# Dataset for ITC fine-tuning\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.missing_images = 0\n",
    "        self.missing_texts = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = row['image_paths'][0] if row['image_paths'] else None\n",
    "        if not image_path or not isinstance(image_path, str) or not os.path.exists(image_path):\n",
    "            image_path = f\"{self.image_dir}/default.jpg\"\n",
    "            self.missing_images += 1\n",
    "            if self.missing_images <= 10:\n",
    "                logger.warning(f\"Missing or invalid image path at index {idx}, using default: {image_path}\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {image_path} at index {idx}: {e}\")\n",
    "            self.missing_images += 1\n",
    "            image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        text = row.get('concatenated_text', row.get('title', 'No description available'))\n",
    "        if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "            text = row.get('description', 'No description available')\n",
    "            self.missing_texts += 1\n",
    "            if self.missing_texts <= 10:\n",
    "                logger.warning(f\"Missing or empty text at index {idx}, using description: {text}\")\n",
    "        \n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def log_data_issues(self):\n",
    "        logger.info(f\"Total missing or invalid images: {self.missing_images}\")\n",
    "        logger.info(f\"Total missing or empty texts: {self.missing_texts}\")\n",
    "\n",
    "# Dataset for VBPR training\n",
    "class VBPRDataset(Dataset):\n",
    "    def __init__(self, user_item_df, item_df, processor):\n",
    "        self.user_item_df = user_item_df\n",
    "        self.item_df = item_df\n",
    "        self.processor = processor\n",
    "        self.items = list(range(len(item_df)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_item_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.user_item_df.iloc[idx]\n",
    "        user_idx = row['user_idx']\n",
    "        pos_item_idx = row['item_idx']\n",
    "        neg_item_idx = np.random.choice([i for i in self.items if i != pos_item_idx])\n",
    "        \n",
    "        pos_item = self.item_df.iloc[pos_item_idx]\n",
    "        neg_item = self.item_df.iloc[neg_item_idx]\n",
    "        \n",
    "        pos_image_path = pos_item['image_paths'][0] if pos_item['image_paths'] else f\"{new_image_dir}/default.jpg\"\n",
    "        neg_image_path = neg_item['image_paths'][0] if neg_item['image_paths'] else f\"{new_image_dir}/default.jpg\"\n",
    "        \n",
    "        try:\n",
    "            pos_image = Image.open(pos_image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading pos image {pos_image_path} at index {idx}: {e}\")\n",
    "            pos_image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        try:\n",
    "            neg_image = Image.open(neg_image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading neg image {neg_image_path} at index {idx}: {e}\")\n",
    "            neg_image = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        pos_inputs = self.processor(images=pos_image, return_tensors=\"pt\", padding=\"max_length\", max_length=77, truncation=True)['pixel_values'].squeeze(0)\n",
    "        neg_inputs = self.processor(images=neg_image, return_tensors=\"pt\", padding=\"max_length\", max_length=77, truncation=True)['pixel_values'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'user_idx': torch.tensor(user_idx, dtype=torch.long),\n",
    "            'pos_item_idx': torch.tensor(pos_item_idx, dtype=torch.long),\n",
    "            'neg_item_idx': torch.tensor(neg_item_idx, dtype=torch.long),\n",
    "            'pos_pixel_values': pos_inputs,\n",
    "            'neg_pixel_values': neg_inputs\n",
    "        }\n",
    "\n",
    "# ITC Loss with temperature\n",
    "class ITCLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(ITCLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, image_features, text_features, labels):\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        logits_per_image = (image_features @ text_features.T) / self.temperature\n",
    "        logits_per_text = (text_features @ image_features.T) / self.temperature\n",
    "        labels = torch.arange(logits_per_image.size(0), device=logits_per_image.device)\n",
    "        loss_i = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_t = F.cross_entropy(logits_per_text, labels)\n",
    "        return (loss_i + loss_t) / 2\n",
    "\n",
    "# VBPR Model\n",
    "class VBPR(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=100, visual_dim=512):\n",
    "        super(VBPR, self).__init__()\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "        self.visual_embeddings = nn.Linear(visual_dim, embedding_dim)\n",
    "        self.bias = nn.Parameter(torch.zeros(num_items))\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.visual_embeddings.weight)\n",
    "        nn.init.zeros_(self.visual_embeddings.bias)\n",
    "\n",
    "    def forward(self, user_idx, item_idx, image_features):\n",
    "        user_emb = self.user_embeddings(user_idx)\n",
    "        item_emb = self.item_embeddings(item_idx)\n",
    "        batch_size = user_idx.size(0)\n",
    "        image_features = image_features.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        visual_emb = self.visual_embeddings(image_features)\n",
    "        scores = (user_emb.unsqueeze(1) * (item_emb + visual_emb)).sum(-1) + self.bias[item_idx]\n",
    "        return scores\n",
    "\n",
    "# Load CLIP model and processor\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=False)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "model.train()\n",
    "\n",
    "# Set differential learning rates and StepLR scheduler\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': model.text_model.parameters(), 'lr': 5e-4},\n",
    "    {'params': model.vision_model.parameters(), 'lr': 1e-4}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Prepare datasets\n",
    "dataset = FashionDataset(df, new_image_dir, processor)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "vbpr_dataset = VBPRDataset(user_item_df, df, processor)\n",
    "vbpr_loader = DataLoader(vbpr_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Fine-tune CLIP with ITC loss\n",
    "itc_loss = ITCLoss(temperature=0.05)\n",
    "num_epochs = 15  # Reduced to 15 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f\"ITC Fine-tuning Epoch {epoch+1}/{num_epochs}\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        image_features = outputs.image_embeds\n",
    "        text_features = outputs.text_embeds\n",
    "\n",
    "        labels = torch.arange(image_features.size(0), device=device)\n",
    "        loss = itc_loss(image_features, text_features, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    scheduler.step()\n",
    "    print(f\"ITC Fine-tuning Epoch {epoch+1}/{num_epochs} Completed, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Log data issues\n",
    "dataset.log_data_issues()\n",
    "\n",
    "# Save fine-tuned CLIP model and processor\n",
    "model.save_pretrained(save_dir)\n",
    "processor.save_pretrained(save_dir)\n",
    "\n",
    "# Generate and normalize embeddings for all items\n",
    "all_image_features = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        image_features = model.get_image_features(pixel_values=pixel_values)\n",
    "        all_image_features.append(image_features.cpu())\n",
    "all_image_features = torch.cat(all_image_features, dim=0)\n",
    "all_image_features = F.normalize(all_image_features, dim=-1)  # Normalize features\n",
    "torch.save(all_image_features, f\"{save_dir}/all_image_features.pt\")\n",
    "\n",
    "# VBPR training\n",
    "num_users = len(user_mapping)\n",
    "num_items = len(df)\n",
    "vbpr_model = VBPR(num_users, num_items).to(device)\n",
    "criterion = nn.MarginRankingLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(vbpr_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10  # Reduced to 10 epochs\n",
    "vbpr_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(vbpr_loader, desc=f\"VBPR Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        user_idx = batch['user_idx'].to(device)\n",
    "        pos_item_idx = batch['pos_item_idx'].to(device)\n",
    "        neg_item_idx = batch['neg_item_idx'].to(device)\n",
    "        pos_pixel_values = batch['pos_pixel_values'].to(device)\n",
    "        neg_pixel_values = batch['neg_pixel_values'].to(device)\n",
    "\n",
    "        pos_image_features = model.get_image_features(pixel_values=pos_pixel_values)\n",
    "        neg_image_features = model.get_image_features(pixel_values=neg_pixel_values)\n",
    "\n",
    "        pos_scores = vbpr_model(user_idx, pos_item_idx, pos_image_features)\n",
    "        neg_scores = vbpr_model(user_idx, neg_item_idx, neg_image_features)\n",
    "\n",
    "        loss = criterion(pos_scores, neg_scores, torch.ones_like(pos_scores))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(vbpr_loader)\n",
    "    print(f\"VBPR Training Epoch {epoch+1}/{num_epochs} Completed, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save VBPR model\n",
    "torch.save(vbpr_model.state_dict(), f\"{save_dir}/vbpr_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2a940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
